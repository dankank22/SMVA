{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing all packages\n",
    "\n",
    "import pandas as pd #to make csv file into table interpreted by Python\n",
    "import os #to traverse folders and file system\n",
    "\n",
    "\n",
    "from sentence_transformers import SentenceTransformer #for vector embedding creation\n",
    "from langchain_community.embeddings import SentenceTransformerEmbeddings\n",
    "import time\n",
    "import nltk #for natural lang processing\n",
    "from nltk.tokenize import sent_tokenize #tokenises string by sentence\n",
    "from langchain_community.vectorstores import FAISS #for vector db\n",
    "from langchain_core.documents import Document #document is a distinct piece of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining transcripts from all video ids to make one final string to be embedded. Completed by traversing all .txt files from \n",
    "# main transcripts folder\n",
    "\n",
    "def get_combined_content(csv_file, folder_path):\n",
    "    \"\"\"\n",
    "    Combines transcript texts from all .txt files in the Main Transcripts folder.\n",
    "\n",
    "    Args:\n",
    "        csv_file (String): The csv path.\n",
    "        folder_path (String): The folder path.\n",
    "\n",
    "    Returns:\n",
    "        String: Combined transcript text.\n",
    "    \"\"\"\n",
    "    input_data = pd.read_csv(csv_file)\n",
    "    video_ids = input_data['Video_ID'].tolist() #creates list of video ids\n",
    "    all_content = \"\"\n",
    "\n",
    "    for video_id in video_ids:\n",
    "        txt_file_path = os.path.join(folder_path, f'{video_id}.txt')\n",
    "        \n",
    "        if os.path.isfile(txt_file_path):\n",
    "            with open(txt_file_path, 'r') as file:\n",
    "                content = file.read() #reading file\n",
    "                all_content += content + \"\\n\" #appending the final string\n",
    "        else:\n",
    "            print(f\"File {txt_file_path} does not exist.\")\n",
    "    \n",
    "    print(\"Task completed.\")\n",
    "    \n",
    "    return all_content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions for chunking and adding chunks for embedding\n",
    "\n",
    "nltk.download('punkt') #module in nltk\n",
    "\n",
    "def chunk_text(text, chunk_size):\n",
    "    #Chunks text into smaller pieces of length chunk_size\n",
    "    return [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)] \n",
    "\n",
    "def process_sentences(sentences, model, chunk_size=256):\n",
    "    #Processes sentences by chunking and encoding.\n",
    "    embeddings = []\n",
    "    chunks_list = []\n",
    "    for sentence in sentences:\n",
    "        chunks = chunk_text(sentence, chunk_size)\n",
    "        chunk_embeddings = model.encode(chunks)  # Use the model from sentence-transformers\n",
    "        embeddings.extend(chunk_embeddings) #adds embeddings\n",
    "        chunks_list.extend(chunks) #adds chunks\n",
    "    return chunks_list, embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using all-MiniLM-L6-v2 model and using above functions\n",
    "\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "start_time = time.time() #start computation time\n",
    "\n",
    "text = get_combined_content(\"mkbdh_information.csv\",\"Main_Transcripts\") #getting final string\n",
    "sentences = sent_tokenize(text)\n",
    "\n",
    "chunks, embeddings = process_sentences(sentences, model) #getting list of chunks and embeddings\n",
    "\n",
    "# Creation of embeddings.txt in 'write mode' to store all embeddings\n",
    "with open('embeddings.txt', 'w') as f:\n",
    "    for i, embedding in enumerate(embeddings): #looping and displaying embedding list for #chunk in each line\n",
    "        f.write(f\"Embedding for chunk {i+1}: {embedding.tolist()}\\n\") #writing it to text file\n",
    "\n",
    "documents = [Document(page_content=chunk) for chunk in chunks] #document object creation where a document is recognised by a chunk\n",
    "print(documents)\n",
    "\n",
    "print(\"Process finished --- %s seconds ---\" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_community.vectorstores import FAISS #for similarity search\n",
    "\n",
    "\n",
    "# Create text-embedding pairs where each chunk and its embedding is a pair\n",
    "text_embedding_pairs = [(chunk, embedding) for chunk, embedding in zip(chunks, embeddings)]\n",
    "\n",
    "# Create vector_db\n",
    "vector_db = FAISS.from_embeddings(text_embeddings=text_embedding_pairs, embedding=model)\n",
    "\n",
    "# Save the vector_db as 'vector_store'\n",
    "vector_db.save_local(folder_path=\"vector_store\", index_name=\"index\")\n",
    "\n",
    "tensor_size = len(embeddings)\n",
    "print(tensor_size)\n",
    "print(\"Vector db created.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_faiss_index(index_path: str,emb_model) -> FAISS:\n",
    "    model_kwargs = {'device':'cpu'} #keyword arguments\n",
    "    model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "    encode_kwargs = {'normalize_embeddings': False} #disable scaling\n",
    "    embeddings = SentenceTransformerEmbeddings(\n",
    "    model_name=model,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")\n",
    "    return FAISS.load_local(index_path, embeddings=embeddings, allow_dangerous_deserialization=True) #bypass safety check\n",
    "# def load_faiss_index(index_path: str,emb_model) -> FAISS:\n",
    "#     model_kwargs = {'device':'cpu'}\n",
    "#     encode_kwargs = {'normalize_embeddings': False}\n",
    "#     embeddings = HuggingFaceEmbeddings(\n",
    "#     model_name=emb_model,\n",
    "#     model_kwargs=model_kwargs,\n",
    "#     encode_kwargs=encode_kwargs\n",
    "# )   \n",
    "#     return FAISS.load_local(index_path, embeddings=embeddings, allow_dangerous_deserialization=True) #bypass safety check\n",
    "def load_faiss_vectordb(db,emb_model):\n",
    "    faiss = load_faiss_index(db,emb_model)\n",
    "    return faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "message_type_response = { #how to handle different types of queries when entered into chatgpt/gemini\n",
    "    \"Greetings Message\" : \"Greet the user by saying Hi or Hello\",\n",
    "    \"Appreciation/Feedback Messages\" : \"Reply with thank you or similar response\",\n",
    "    \"Questions Asked About the Content\" : \"Refer the context provided below\",\n",
    "    \"Questions Asked Out of Context but Relevant to the Influencer\" : \"Refer online material and generate response\",\n",
    "    \"Questions Asked but Irrelevant to the Influencer\": \"Don't answer\",\n",
    "    \"Spam Messages\": \"Don't answer\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.document_loaders.base import Document\n",
    "from typing import List\n",
    "\n",
    "#start of RAG stage\n",
    "\n",
    "#function for similarity search using an arbitrary number of top documents(5) and take query as a string and return a list of docs\n",
    "\n",
    "def similarity_search(faiss_index: FAISS, query: str, k: int = 5) -> List[Document]:\n",
    "    docs = faiss_index.similarity_search(query, k=k)\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#library that converts text to speech\n",
    "import pyttsx3\n",
    "def text_to_speech(text, filename): #takes in text and name of file to save speech in\n",
    "    text_speech = pyttsx3.init()\n",
    "    voices = text_speech.getProperty('voices') #retrives list of voices\n",
    "    text_speech.setProperty('voice', voices[0].id) #chooses a particular voice\n",
    "    text_speech.save_to_file(text, filename) #saves speech as .wav file\n",
    "    text_speech.runAndWait() #execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI #importing gemini model from google \n",
    "\n",
    "def load_model():\n",
    "    llm = ChatGoogleGenerativeAI(model=\"gemini-pro\",api_key=\"MY_API_KEY\") #loads model with my own api key\n",
    "    return llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_retrieval_results(context: List[Document], point: str) -> str:\n",
    "    context = [doc.page_content for doc in context] #retrive context for query\n",
    "    context = \" \".join(context)\n",
    "    # creation of prompt that is relevant\n",
    "    #263 tokens\n",
    "    prompt =  f''' Assume you are a Human tech content creator named MKBHD/ Marques Brownlee and you receive different comments and queries. \n",
    "                    Ensure to give response such that it looks like a human has answered it and not a bot.\n",
    "                    Read the following text based on the following query and give response. Different types of messages are provided here as key of the dictionary and the expected response as respective value. Give the response accordingly.\n",
    "                    In case, the query is asking response for a specific timeline, the context provided below has upload date of the data (release date of device would also be around it), refer it while giving response. Don't give old devices as response.\n",
    "                    Message_Response: {message_type_response}            \n",
    "                    Query is: {point}\n",
    "                    Text: {context}\n",
    "                    When the query is related to the context provided, don't give any irrelevant information. In other cases, refer the Message_Response dictionary and generate the response by following those guidelines; don't give the value directly. \n",
    "                    \n",
    "                    More guidelines:\n",
    "                    1. Never mention in the response that the context is being refered, and give the response directly.\n",
    "                    2. Avoid including feel free to ask in the response when there is an actual response to the query. \n",
    "                    3. Avoid greetings if the query doesn't consist of only greetings.\n",
    "                    4. Understand the language and words chosen by the content creator in the context provided and ensure that the response follows it.\n",
    "                '''\n",
    "    # 210 tokens\n",
    "    # prompt =  f''' Assume you are a human tech content creator who receives various comments and queries \n",
    "    #                 Read the text {context} based on the query {point} and ensure to respond like a human, not a bot\n",
    "    #                 Different types of messages are provided here as keys of the dictionary with the expected response as the respective value.Respond accordingly\n",
    "    #                 If the query asks for a response related to a specific timeline, refer to the provided upload date (which is around the device's release date) when responding, avoid mentioning old devices\n",
    "\n",
    "    #                 When the query relates to the provided context, avoid giving irrelevant information. For other cases, use the Message_Response {message_type_response} dictionary to generate the response according to the guidelines; donâ€™t provide the value directly\n",
    "                    \n",
    "    #                 More guidelines:\n",
    "    #                 - You are MKBHD or Marques Brownlee\n",
    "    #                 - Never mention that the context is being referenced; provide the response directly\n",
    "    #                 - Address the query directly, avoid \"feel free to ask\" in responses\n",
    "    #                 - Avoid greetings unless the query consists solely of greetings\n",
    "    #                 - Understand the language and tone used by the content creator in the provided context and ensure your response matches it\n",
    "    #             '''\n",
    "    \n",
    "    model = load_model()\n",
    "    response = model.invoke(prompt) #generation of prompt\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#access embeddings and use them\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "def load_faiss_index(index_path: str, model) -> FAISS:\n",
    "    model_kwargs = {'device': 'cpu'}\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=model, model_kwargs=model_kwargs)\n",
    "    return FAISS.load_local(index_path, embeddings=embeddings, allow_dangerous_deserialization=True)\n",
    "\n",
    "def load_faiss_vectordb(db, emb_model):\n",
    "    faiss = load_faiss_index(db, emb_model)\n",
    "    return faiss\n",
    "\n",
    "faiss_index = load_faiss_vectordb('vector_store', 'BAAI/bge-large-en-v1.5') #loading faiss index from 'vector_db'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get query from user - single response\n",
    "query = input(\"Please enter your query: \") \n",
    "print(query)\n",
    "\n",
    "docs = similarity_search(faiss_index, query, 5)\n",
    "result = get_retrieval_results(context=docs, point=query) #generates result from retrieved doc\n",
    "print(\"Results: \", result)\n",
    "\n",
    "# Get query from user - multi-response\n",
    "# while True:\n",
    "#     query = input(\"Please enter your query: \")\n",
    "#     print(\"User: \", query)\n",
    "#     docs = similarity_search(faiss_index, query, 5)\n",
    "#     result = get_retrieval_results(context=docs, point=query) # generates result from retrieved doc\n",
    "#     print(\"MKBHD: \", result)\n",
    "    \n",
    "#     if query.lower() == \"thank you mkbhd\":\n",
    "#         print(\"You're welcome! Goodbye!\")\n",
    "#         break\n",
    "\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs #returns list of relevant docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"output.wav\"\n",
    "text_to_speech(result, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Notes\n",
    "\n",
    "- EXTEND FOR MORE EMBEDDINGS WITHOUT REGENERATION EVERYTIME\n",
    "- LEARN FAISS INTERNAL MECHANISM\n",
    "- PROMPT ENGINEERING FOR QUALITY RESPONSE AND TO USE OPENAI IN LESS TOKENS(IMP)\n",
    "- CREATING MORE TEST QUERIES FOR ACCURACY TESTING (IMP)\n",
    "- CREATE CHAT INTERFACE OR SYSTEM TO USE USER RESPONSE AS CONTEXT FOR NEW RESPONSE\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
